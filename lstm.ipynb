{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df_partitions = pd.read_csv(\"c3_muse_stress/metadata/partition.csv\")\n",
    "features_path = \"c3_muse_stress/feature_segments\"\n",
    "labels_path = \"c3_muse_stress/label_segments\"\n",
    "\n",
    "# Load a subject's data\n",
    "def load_data(id):\n",
    "    df_bpm = pd.read_csv(f'{features_path}/BPM/{id}.csv').drop(columns=['segment_id'])\n",
    "    df_ecg = pd.read_csv(f'{features_path}/ECG/{id}.csv').drop(columns=['segment_id'])\n",
    "    df_resp = pd.read_csv(f'{features_path}/resp/{id}.csv').drop(columns=['segment_id'])\n",
    "    df_arousal = pd.read_csv(f'{labels_path}/arousal/{id}.csv').drop(columns=['segment_id'])\n",
    "    df_valence = pd.read_csv(f'{labels_path}/valence/{id}.csv').drop(columns=['segment_id'])\n",
    "    df_arousal.columns = ['timestamp', 'arousal']\n",
    "    df_valence.columns = ['timestamp', 'valence']\n",
    "    return df_bpm, df_ecg, df_resp, df_arousal, df_valence\n",
    "\n",
    "def merge_dataframes(df_bpm, df_ecg, df_resp, df_arousal, df_valence):\n",
    "    df = pd.merge_asof(df_bpm, df_ecg, on='timestamp')\n",
    "    df = pd.merge_asof(df, df_resp, on='timestamp')\n",
    "    df = pd.merge_asof(df, df_arousal, on='timestamp')\n",
    "    df = pd.merge_asof(df, df_valence, on='timestamp')\n",
    "    return df.dropna()\n",
    "\n",
    "# Split data into train and devel\n",
    "train_ids = df_partitions[df_partitions['Proposal'] == 'train']['Id']\n",
    "devel_ids = df_partitions[df_partitions['Proposal'] == 'devel']['Id']\n",
    "\n",
    "train_data = [merge_dataframes(*load_data(id)) for id in train_ids]\n",
    "devel_data = [merge_dataframes(*load_data(id)) for id in devel_ids]\n",
    "\n",
    "train_data = pd.concat(train_data)\n",
    "devel_data = pd.concat(devel_data)\n",
    "\n",
    "# Extract features and labels\n",
    "X_train = train_data[['timestamp', 'BPM', 'ECG', 'resp']].values\n",
    "y_train = train_data[['timestamp', 'arousal', 'valence']].values\n",
    "X_devel = devel_data[['timestamp', 'BPM', 'ECG', 'resp']].values\n",
    "y_devel = devel_data[['timestamp', 'arousal', 'valence']].values\n",
    "\n",
    "# Remove timestamp column before normalization\n",
    "X_train = X_train[:, 1:]\n",
    "y_train = y_train[:, 1:]\n",
    "X_devel = X_devel[:, 1:]\n",
    "y_devel = y_devel[:, 1:]\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_devel = scaler.transform(X_devel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "3      4\n",
       "4      5\n",
       "7      8\n",
       "8      9\n",
       "9     10\n",
       "10    11\n",
       "14    16\n",
       "16    18\n",
       "18    20\n",
       "21    23\n",
       "22    24\n",
       "25    29\n",
       "26    30\n",
       "27    31\n",
       "28    32\n",
       "29    33\n",
       "31    35\n",
       "32    36\n",
       "34    38\n",
       "38    42\n",
       "39    43\n",
       "40    44\n",
       "42    46\n",
       "43    47\n",
       "44    48\n",
       "45    50\n",
       "47    52\n",
       "48    53\n",
       "53    58\n",
       "54    59\n",
       "55    60\n",
       "57    62\n",
       "58    63\n",
       "60    65\n",
       "61    66\n",
       "63    68\n",
       "64    69\n",
       "66    71\n",
       "67    72\n",
       "68    73\n",
       "Name: Id, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class PhysiologicalLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_prob=0.5):\n",
    "        super(PhysiologicalLSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.dropout(out[:, -1, :])\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "input_size = 3\n",
    "hidden_size = 50\n",
    "num_layers = 2\n",
    "output_size = 2\n",
    "dropout_prob = 0.5\n",
    "\n",
    "model = PhysiologicalLSTMModel(input_size, hidden_size, num_layers, output_size, dropout_prob)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class PhysiologicalSequenceDataset(Dataset):\n",
    "    def __init__(self, X, y, sequence_length):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X) - self.sequence_length + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx:idx+self.sequence_length], self.y[idx+self.sequence_length-1]\n",
    "\n",
    "sequence_length = 15  # Example sequence length, adjust as needed\n",
    "\n",
    "train_dataset = PhysiologicalSequenceDataset(X_train, y_train, sequence_length)\n",
    "devel_dataset = PhysiologicalSequenceDataset(X_devel, y_devel, sequence_length)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "devel_loader = DataLoader(devel_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=20, alpha=1.0):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
    "\n",
    "def evaluate_model(model, devel_loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in devel_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "    devel_loss = running_loss / len(devel_loader.dataset)\n",
    "    print(f'Validation Loss: {devel_loss:.4f}')\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=20)\n",
    "print()\n",
    "evaluate_model(model, devel_loader, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, _ in devel_loader:\n",
    "        outputs = model(inputs)\n",
    "        predictions.append(outputs.numpy())\n",
    "\n",
    "predictions = np.vstack(predictions)\n",
    "\n",
    "ground_truth = np.vstack([labels.numpy() for _, labels in devel_loader])\n",
    "\n",
    "pred_df = pd.DataFrame(predictions, columns=['Predicted Arousal', 'Predicted Valence'])\n",
    "gt_df = pd.DataFrame(ground_truth, columns=['True Arousal', 'True Valence'])\n",
    "\n",
    "timestamps = pd.date_range(start='2022-01-01', periods=len(pred_df), freq='S')\n",
    "\n",
    "pred_df['timestamp'] = timestamps\n",
    "gt_df['timestamp'] = timestamps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# window_size = 1\n",
    "# def smooth_signal(signal, window_size=5):\n",
    "#     return np.convolve(signal, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "# smooth_arousal = smooth_signal(pred_df['Predicted Arousal'], window_size=window_size)\n",
    "# smooth_valence = smooth_signal(pred_df['Predicted Valence'], window_size=window_size)\n",
    "\n",
    "# # Truncate the timestamps to match the length of the smoothed signals\n",
    "# pred_df = pred_df.iloc[window_size-1:].copy()\n",
    "# pred_df['Smoothed Arousal'] = smooth_arousal\n",
    "# pred_df['Smoothed Valence'] = smooth_valence\n",
    "\n",
    "# gt_df = gt_df.iloc[:len(pred_df)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Create traces for arousal\n",
    "fig.add_trace(go.Scatter(x=gt_df['timestamp'], y=gt_df['True Arousal'], \n",
    "                         mode='lines', name='True Arousal'))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=pred_df['timestamp'], y=pred_df['Predicted Arousal'], \n",
    "                         mode='lines', name='Predicted Arousal'))\n",
    "\n",
    "# Create traces for valence\n",
    "fig.add_trace(go.Scatter(x=gt_df['timestamp'], y=gt_df['True Valence'], \n",
    "                         mode='lines', name='True Valence'))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=pred_df['timestamp'], y=pred_df['Predicted Valence'], \n",
    "                         mode='lines', name='Predicted Valence'))\n",
    "\n",
    "fig.update_layout(title='True vs Predicted Arousal and Valence',\n",
    "                  xaxis_title='Timestamp',\n",
    "                  yaxis_title='Value',\n",
    "                  legend=dict(x=0.1, y=1.1),\n",
    "                  legend_orientation=\"h\")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                               stride=stride, padding=padding, dilation=dilation)\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                               stride=stride, padding=padding, dilation=dilation)\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.fc = nn.Linear(num_channels[-1], output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)  # Transpose to (batch_size, channels, seq_length)\n",
    "        x = self.network(x)\n",
    "        x = x[:, :, -1]  # Take the last element in the sequence\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "num_inputs = 3\n",
    "num_channels = [50] * 2\n",
    "kernel_size = 2\n",
    "dropout = 0.2\n",
    "output_size = 2\n",
    "\n",
    "model = TemporalConvNet(num_inputs, num_channels, kernel_size=kernel_size, dropout=dropout)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# Continue with training and evaluation as previously defined\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=20, alpha=1.0)\n",
    "evaluate_model(model, devel_loader, criterion, alpha=1.0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
